//! AMD SEV shared memory management for VirtIO.
//!
//! For AMD SEV (Secure Encrypted Virtualization), memory pages are encrypted by default
//! when the C-Bit is set in page table entries. VirtIO devices require unencrypted
//! (shared) memory regions that both the guest and host can access.
//!
//! This module provides:
//! - A pre-allocated shared memory pool without C-Bit encryption
//! - Allocation/deallocation APIs for VirtIO DMA buffers
//! - Implementation of the `PsciIf` interface

use axplat::psci::PsciIf;
use log::{debug, info, warn};
use kspin::SpinNoIrq;
use lazyinit::LazyInit;

// Import shared memory configuration from platform config
// These are generated by axconfig-macros from axconfig.toml
use crate::config::plat::SHARED_MEM_BASE;
use crate::config::plat::SHARED_MEM_SIZE;

const PAGE_SIZE: usize = 0x1000;
const MAX_PAGES: usize = SHARED_MEM_SIZE / PAGE_SIZE;
const BITMAP_SIZE: usize = (MAX_PAGES + 63) / 64;

/// Shared memory allocator for VirtIO DMA buffers.
///
/// This allocator manages a fixed region of physical memory that is mapped
/// without the C-Bit, making it accessible to both guest and host.
struct SharedMemAllocator {
    /// Bitmap tracking allocated pages (1 = allocated, 0 = free)
    bitmap: [u64; BITMAP_SIZE],
    /// Hint for next allocation search
    next_hint: usize,
    /// Number of allocated pages
    allocated_pages: usize,
}

impl SharedMemAllocator {
    const fn new() -> Self {
        Self {
            bitmap: [0; BITMAP_SIZE],
            next_hint: 0,
            allocated_pages: 0,
        }
    }

    /// Allocates contiguous pages from the shared memory pool.
    ///
    /// Returns the physical address of the allocated region, or `None` if
    /// there is insufficient contiguous space.
    fn alloc_pages(&mut self, pages: usize) -> Option<usize> {
        if pages == 0 {
            return None;
        }

        // For single page allocation, use simple bitmap search
        if pages == 1 {
            return self.alloc_single_page();
        }

        // For multi-page allocation, search for contiguous free pages
        self.alloc_contiguous_pages(pages)
    }

    fn alloc_single_page(&mut self) -> Option<usize> {
        // Search from hint position
        for i in self.next_hint..MAX_PAGES {
            if self.is_page_free(i) {
                self.set_page_allocated(i);
                self.next_hint = i + 1;
                self.allocated_pages += 1;
                return Some(SHARED_MEM_BASE + i * PAGE_SIZE);
            }
        }

        // Wrap around search
        for i in 0..self.next_hint {
            if self.is_page_free(i) {
                self.set_page_allocated(i);
                self.next_hint = i + 1;
                self.allocated_pages += 1;
                return Some(SHARED_MEM_BASE + i * PAGE_SIZE);
            }
        }

        None
    }

    fn alloc_contiguous_pages(&mut self, pages: usize) -> Option<usize> {
        let mut start = 0;
        let mut count = 0;

        for i in 0..MAX_PAGES {
            if self.is_page_free(i) {
                if count == 0 {
                    start = i;
                }
                count += 1;
                if count == pages {
                    // Found enough contiguous pages
                    for j in start..start + pages {
                        self.set_page_allocated(j);
                    }
                    self.allocated_pages += pages;
                    self.next_hint = start + pages;
                    return Some(SHARED_MEM_BASE + start * PAGE_SIZE);
                }
            } else {
                count = 0;
            }
        }

        None
    }

    /// Frees previously allocated pages back to the pool.
    fn free_pages(&mut self, paddr: usize, pages: usize) {
        if paddr < SHARED_MEM_BASE || paddr >= SHARED_MEM_BASE + SHARED_MEM_SIZE {
            warn!(
                "free_pages: address {:#x} is outside shared memory region",
                paddr
            );
            return;
        }

        let start_page = (paddr - SHARED_MEM_BASE) / PAGE_SIZE;
        for i in 0..pages {
            let page_idx = start_page + i;
            if page_idx < MAX_PAGES {
                self.set_page_free(page_idx);
                self.allocated_pages = self.allocated_pages.saturating_sub(1);
            }
        }

        // Update hint to freed region for potential reuse
        if start_page < self.next_hint {
            self.next_hint = start_page;
        }
    }

    #[inline]
    fn is_page_free(&self, page_idx: usize) -> bool {
        let word_idx = page_idx / 64;
        let bit_idx = page_idx % 64;
        (self.bitmap[word_idx] & (1u64 << bit_idx)) == 0
    }

    #[inline]
    fn set_page_allocated(&mut self, page_idx: usize) {
        let word_idx = page_idx / 64;
        let bit_idx = page_idx % 64;
        self.bitmap[word_idx] |= 1u64 << bit_idx;
    }

    #[inline]
    fn set_page_free(&mut self, page_idx: usize) {
        let word_idx = page_idx / 64;
        let bit_idx = page_idx % 64;
        self.bitmap[word_idx] &= !(1u64 << bit_idx);
    }
}

static SHARED_ALLOCATOR: LazyInit<SpinNoIrq<SharedMemAllocator>> = LazyInit::new();

/// Initializes the shared memory allocator.
///
/// Must be called before any shared memory allocation.
pub fn init() {
    SHARED_ALLOCATOR.init_once(SpinNoIrq::new(SharedMemAllocator::new()));
    info!(
        "SEV shared memory pool initialized: base={:#x}, size={:#x} ({} pages)",
        SHARED_MEM_BASE, SHARED_MEM_SIZE, MAX_PAGES
    );
}

/// Allocates shared (unencrypted) pages for VirtIO DMA.
///
/// Returns the physical address of the allocated region.
pub fn alloc_shared_pages(pages: usize) -> Option<usize> {
    let result = SHARED_ALLOCATOR.lock().alloc_pages(pages);
    if result.is_none() {
        warn!(
            "alloc_shared_pages: failed to allocate {} pages from shared memory pool",
            pages
        );
    }
    result
}

/// Frees shared pages back to the pool.
pub fn free_shared_pages(paddr: usize, pages: usize) {
    SHARED_ALLOCATOR.lock().free_pages(paddr, pages);
}

/// Returns whether an address is within the shared memory region.
pub fn is_shared_memory(paddr: usize) -> bool {
    paddr >= SHARED_MEM_BASE && paddr < SHARED_MEM_BASE + SHARED_MEM_SIZE
}

/// Returns the shared memory region range.
pub fn shared_memory_range() -> (usize, usize) {
    (SHARED_MEM_BASE, SHARED_MEM_BASE + SHARED_MEM_SIZE)
}

struct PsciImpl;

#[impl_plat_interface]
impl PsciIf for PsciImpl {
    /// Marks a DMA buffer as shared with the host.
    ///
    /// For AMD SEV, the shared memory region is pre-allocated without the C-Bit,
    /// so this function is primarily for logging/debugging purposes.
    /// The actual "sharing" happens through page table configuration.
    fn share_dma_buffer(phys_addr: usize, size: usize) {
        debug!(
            "share_dma_buffer: paddr={:#x}, size={:#x}, in_shared_region={}",
            phys_addr,
            size,
            is_shared_memory(phys_addr)
        );
        // For AMD SEV, the shared memory pool is already mapped without C-Bit.
        // No additional action needed for buffers allocated from the shared pool.
        // If dynamic sharing is needed, GHCB protocol would be used here.
    }

    /// Marks a DMA buffer as private (no longer shared with host).
    ///
    /// For AMD SEV, this is informational as the memory layout is static.
    fn unshare_dma_buffer(phys_addr: usize, size: usize) {
        debug!(
            "unshare_dma_buffer: paddr={:#x}, size={:#x}",
            phys_addr, size
        );
        // For AMD SEV with static shared memory pool, no action needed.
    }
}
